@inproceedings{na,
  title={First Steps Towards a Runtime Analysis of Neuroevolution},
  author={Paul Fischer and Larsen, {Emil Lundt} and Carsten Witt},
  year={2023},
  pages={"61--72"},
  booktitle={Proceedings of the 17th ACM/SIGEVO Conference on Foundations of Genetic Algorithms"},
  publisher={Association for Computing Machinery"}
}

@inproceedings{bna,
  title={A Runtime analysis of Bias-invariant Neuroevolution and Dynamic Fitness Evaluation},
  booktitle={Proceedings of the 17th ACM/SIGEVO Conference on Foundations of Genetic Algorithms"},
  year={To appear}
}

@inproceedings{snowballing,
author = {Wohlin, Claes},
title = {Guidelines for snowballing in systematic literature studies and a replication in software engineering},
year = {2014},
publisher = {Association for Computing Machinery},
booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {38},
numpages = {10},
series = {EASE '14}
}

@article{systematic_review,
title = {Guidelines for conducting systematic mapping studies in software engineering: An update},
journal = {Information and Software Technology},
volume = {64},
pages = {1-18},
year = {2015},
author = {Kai Petersen and Sairam Vakkalanka and Ludwik Kuzniarz},
}

@ARTICLE{neat,
  author={Stanley, Kenneth O. and Miikkulainen, Risto},
  journal={Evolutionary Computation},
  title={Evolving Neural Networks through Augmenting Topologies},
  year={2002},
  volume={10},
  number={2},
  pages={99-127},
}

@book{proben,
  author       = {Prechelt, Lutz},
  year         = {1994},
  title        = {PROBEN 1 - a set of benchmarks and benchmarking rules for neural network training algorithms},
  language     = {english},
  note         = {Karlsruhe 1994. (Technical report. Fakultät für Informatik, Universität Karlsruhe. 1994,21.)}
}

@INPROCEEDINGS{pole_balancing,
  author={Wieland, A.P.},
  booktitle={IJCNN-91-Seattle International Joint Conference on Neural Networks},
  title={Evolving neural network controllers for unstable systems},
  year={1991},
  volume={ii},
  number={},
  pages={667-673 vol.2},
}

@misc{sgd,
      title={Optimization for deep learning: theory and algorithms},
      author={Ruoyu Sun},
      year={2019},
      eprint={1912.08957},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{adam,
      title={Adam: A Method for Stochastic Optimization},
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@INPROCEEDINGS{speech,
  author={Deng, Li and Hinton, Geoffrey and Kingsbury, Brian},
  booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
  title={New types of deep neural network learning for speech recognition and related applications: an overview},
  year={2013},
  volume={},
  number={},
  pages={8599-8603},
  keywords={Neural networks;Speech recognition;Acoustics;Hidden Markov models;Speech;Training;Optimization;deep neural network;convolutional neural network;recurrent neural network;optimization;spectrogram features;multitask;multilingual;speech recognition;music processing},
}

@inproceedings{attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 volume = {30},
 year = {2017}
}

@inproceedings{cnn,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 volume = {25},
 year = {2012}
}

@INPROCEEDINGS{layers,
  author={Uzair, Muhammad and Jamil, Noreen},
  booktitle={2020 IEEE 23rd International Multitopic Conference (INMIC)},
  title={Effects of Hidden Layers on the Efficiency of Neural networks},
  year={2020},
  volume={},
  number={},
  pages={1-6},
  keywords={Neural network;Hidden layers;Neurons;Accuracy;Time complexity},
}

@misc{openai_es,
      title={Evolution Strategies as a Scalable Alternative to Reinforcement Learning},
      author={Tim Salimans and Jonathan Ho and Xi Chen and Szymon Sidor and Ilya Sutskever},
      year={2017},
      eprint={1703.03864},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{ea_applications,
  title={Evolutionary algorithms and their applications to engineering problems},
  author={Adam Słowik and Halina Kwasnicka},
  journal={Neural Computing and Applications},
  year={2020},
  volume={32},
  pages={12363 - 12379},
}

@ARTICLE{neuroevolution_trends,
  author={Galván, Edgar and Mooney, Peter},
  journal={IEEE Transactions on Artificial Intelligence},
  title={Neuroevolution in Deep Neural Networks: Current Trends and Future Challenges},
  year={2021},
  volume={2},
  number={6},
  pages={476-493},
  keywords={Computer architecture;Deep learning;Training data;Neural networks;Machine learning algorithms;Optimization;Feature extraction;Neuroscience;Evolutionary algorithms;Deep learning (DL);deep neural networks (DNNs);evolutionary algorithms (EAs);machine learning;neuroevolution},
}

@article{nas_survey,
   title={A Survey on Evolutionary Neural Architecture Search},
   volume={34},
   ISSN={2162-2388},
   number={2},
   journal={IEEE Transactions on Neural Networks and Learning Systems},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Liu, Yuqiao and Sun, Yanan and Xue, Bing and Zhang, Mengjie and Yen, Gary G. and Tan, Kay Chen},
   year={2023},
   month=feb, pages={550–570} }

@article{neuroevolution_rl,
title = {Neuroevolution strategies for episodic reinforcement learning},
journal = {Journal of Algorithms},
volume = {64},
number = {4},
pages = {152-168},
year = {2009},
note = {Special Issue: Reinforcement Learning},
issn = {0196-6774},
author = {Verena Heidrich-Meisner and Christian Igel},
keywords = {Reinforcement learning, Evolution strategy, Covariance matrix adaptation, Partially observable Markov decision process, Direct policy search},
abstract = {Because of their convincing performance, there is a growing interest in using evolutionary algorithms for reinforcement learning. We propose learning of neural network policies by the covariance matrix adaptation evolution strategy (CMA-ES), a randomized variable-metric search algorithm for continuous optimization. We argue that this approach, which we refer to as CMA Neuroevolution Strategy (CMA-NeuroES), is ideally suited for reinforcement learning, in particular because it is based on ranking policies (and therefore robust against noise), efficiently detects correlations between parameters, and infers a search direction from scalar reinforcement signals. We evaluate the CMA-NeuroES on five different (Markovian and non-Markovian) variants of the common pole balancing problem. The results are compared to those described in a recent study covering several RL algorithms, and the CMA-NeuroES shows the overall best performance.}
}
@article{neuroevolution_survey,
author = {Stanley, Kenneth and Clune, Jeff and Lehman, Joel and Miikkulainen, Risto},
year = {2019},
month = {01},
pages = {},
title = {Designing neural networks through neuroevolution},
volume = {1},
journal = {Nature Machine Intelligence},
}

@misc{gym,
      title={OpenAI Gym},
      author={Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
      year={2016},
      eprint={1606.01540},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@ARTICLE{cmaes,
  author={Hansen, Nikolaus and Ostermeier, Andreas},
  journal={Evolutionary Computation},
  title={Completely Derandomized Self-Adaptation in Evolution Strategies},
  year={2001},
  volume={9},
  number={2},
  pages={159-195},
  keywords={Evolution strategy;self-adaptation;strategy parameter control;step size control;de-randomization;derandomized self-adaptation;covariance matrix adaptation;evolution path;cumulation;cumulative path length control},
}

@article{backpropagation,
  title={Learning representations by back-propagating errors},
  author={David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  journal={Nature},
  year={1986},
  volume={323},
  pages={533-536},
}

@misc{activation,
      title={Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark},
      author={Shiv Ram Dubey and Satish Kumar Singh and Bidyut Baran Chaudhuri},
      year={2022},
      eprint={2109.14545},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{relu,
      title={Deep Learning using Rectified Linear Units (ReLU)},
      author={Abien Fred Agarap},
      year={2019},
      eprint={1803.08375},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@article{approximation,
title = {Multilayer feedforward networks are universal approximators},
journal = {Neural Networks},
volume = {2},
number = {5},
pages = {359-366},
year = {1989},
issn = {0893-6080},
author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}

@article{overfitting,
year = {2019},
month = {feb},
publisher = {IOP Publishing},
volume = {1168},
number = {2},
pages = {022022},
author = {Xue Ying},
title = {An Overview of Overfitting and its Solutions},
journal = {Journal of Physics: Conference Series},
abstract = {Overfitting is a fundamental issue in supervised machine learning which prevents us from perfectly generalizing the models to well fit observed data on training data, as well as unseen data on testing set. Because of the presence of noise, the limited size of training set, and the complexity of classifiers, overfitting happens. This paper is going to talk about overfitting from the perspectives of causes and solutions. To reduce the effects of overfitting, various strategies are proposed to address to these causes: 1) “early-stopping” strategy is introduced to prevent overfitting by stopping training before the performance stops optimize; 2) “network-reduction” strategy is used to exclude the noises in training set; 3) “data-expansion” strategy is proposed for complicated models to fine-tune the hyper-parameters sets with a great amount of data; and 4) “regularization” strategy is proposed to guarantee models performance to a great extent while dealing with real world issues by feature-selection, and by distinguishing more useful and less useful features.}
}

@article{vanishing_gradient,
author = {Hochreiter, Sepp},
title = {The Vanishing Gradient Problem During Learning Recurrent Neural Nets  and Problem Solutions},
journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
volume = {06},
number = {02},
pages = {107-116},
year = {1998},
}

@Article{loss_survey,
  author={Qi Wang and Yue Ma and Kun Zhao and Yingjie Tian},
  title={{A Comprehensive Survey of Loss Functions in Machine Learning}},
  journal={Annals of Data Science},
  year=2022,
  volume={9},
  number={2},
  pages={187-212},
  month={April},
  keywords={Loss function; Machine learning; Deep learning; Survey},
  abstract={ As one of the important research topics in machine learning, loss function plays an important role in the construction of machine learning algorithms and the improvement of their performance, which has been concerned and explored by many researchers. But it still has a big gap to summarize, analyze and compare the classical loss functions. Therefore, this paper summarizes and analyzes 31 classical loss functions in machine learning. Specifically, we describe the loss functions from the aspects of traditional machine learning and deep learning respectively. The former is divided into classification problem, regression problem and unsupervised learning according to the task type. The latter is subdivided according to the application scenario, and here we mainly select object detection and face recognition to introduces their loss functions. In each task or application, in addition to analyzing each loss function from formula, meaning, image and algorithm, the loss functions under the same task or application are also summarized and compared to deepen the understanding and provide help for the selection and improvement of loss function.},
}

@misc{on_loss,
      title={On Loss Functions for Deep Neural Networks in Classification},
      author={Katarzyna Janocha and Wojciech Marian Czarnecki},
      year={2017},
      eprint={1702.05659},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@ARTICLE{cnn_survey,
  author={Rawat, Waseem and Wang, Zenghui},
  journal={Neural Computation},
  title={Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review},
  year={2017},
  volume={29},
  number={9},
  pages={2352-2449},
  keywords={},
}

@article{spam_survey,
title = {A review of machine learning approaches to Spam filtering},
journal = {Expert Systems with Applications},
volume = {36},
number = {7},
pages = {10206-10222},
year = {2009},
issn = {0957-4174},
author = {Thiago S. Guzella and Walmir M. Caminhas},
keywords = {Spam filtering, Online learning, Bag-of-words (BoW), Naive Bayes, Image Spam},
abstract = {In this paper, we present a comprehensive review of recent developments in the application of machine learning algorithms to Spam filtering, focusing on both textual- and image-based approaches. Instead of considering Spam filtering as a standard classification problem, we highlight the importance of considering specific characteristics of the problem, especially concept drift, in designing new filters. Two particularly important aspects not widely recognized in the literature are discussed: the difficulties in updating a classifier based on the bag-of-words representation and a major difference between two early naive Bayes models. Overall, we conclude that while important advancements have been made in the last years, several aspects remain to be explored, especially under more realistic evaluation settings.}
}

@misc{classification_metrics,
   author = "Wikipedia",
   title = "{Evaluation of binary classifiers} --- {W}ikipedia{,} The Free Encyclopedia",
   year = "2024",
   howpublished = {\url{http://en.wikipedia.org/w/index.php?title=Evaluation\%20of\%20binary\%20classifiers&oldid=1219371150}},
   note = "[Online; accessed 01-June-2024]"
 }

@Inbook{unsupervised_learning,
author="Ghahramani, Zoubin",
editor="Bousquet, Olivier
and von Luxburg, Ulrike
and R{\"a}tsch, Gunnar",
title="Unsupervised Learning",
bookTitle="Advanced Lectures on Machine Learning: ML Summer Schools 2003, Canberra, Australia, February 2 - 14, 2003, T{\"u}bingen, Germany, August 4 - 16, 2003, Revised Lectures",
year="2004",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="72--112",
abstract="We give a tutorial and overview of the field of unsupervised learning from the perspective of statistical modeling. Unsupervised learning can be motivated from information theoretic and Bayesian principles. We briefly review basic models in unsupervised learning, including factor analysis, PCA, mixtures of Gaussians, ICA, hidden Markov models, state-space models, and many variants and extensions. We derive the EM algorithm and give an overview of fundamental concepts in graphical models, and inference algorithms on graphs. This is followed by a quick tour of approximate Bayesian inference, including Markov chain Monte Carlo (MCMC), Laplace approximation, BIC, variational approximations, and expectation propagation (EP). The aim of this chapter is to provide a high-level view of the field. Along the way, many state-of-the-art ideas and future directions are also reviewed.",
}

@inproceedings{dimensionality_review,
  title={Dimensionality Reduction: A Comparative Review},
  author={Laurens van der Maaten and Eric O. Postma and Jaap van den Herik},
  year={2008},
}

@misc{dimensionality_survey,
      title={A survey of dimensionality reduction techniques},
      author={C. O. S. Sorzano and J. Vargas and A. Pascual Montano},
      year={2014},
      eprint={1403.2877},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{rl_survey,
      title={Reinforcement Learning: A Survey},
      author={L. P. Kaelbling and M. L. Littman and A. W. Moore},
      year={1996},
      eprint={cs/9605103},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@INPROCEEDINGS{genetic_alg_review,
  author={Lambora, Annu and Gupta, Kunal and Chopra, Kriti},
  booktitle={2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon)},
  title={Genetic Algorithm- A Literature Review},
  year={2019},
  volume={},
  number={},
  pages={380-384},
  keywords={Genetic algorithms;Biological cells;Optimization;Sociology;Statistics;Encoding;Genetics;Genetic Algorithm;Inheritance;Mutation;Selection;Crossover},
}

@Inbook{genetic_alg,
author="Mirjalili, Seyedali",
title="Genetic Algorithm",
bookTitle="Evolutionary Algorithms and Neural Networks: Theory and Applications",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="43--55",
abstract="Genetic Algorithm (GA) is one of the first population-based stochastic algorithm proposed in the history. Similar to other EAs, the main operators of GA are selection, crossover, and mutation. This chapter briefly presents this algorithm and applies it to several case studies to observe its performance.",
}

@inproceedings{cartesian_gp,
author = {Miller, Julian and Turner, Andrew},
title = {Cartesian Genetic Programming},
year = {2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Cartesian Genetic Programming (CGP) is a well-known form of Genetic Programming developed by Julian Miller in 1999-2000. In its classic form, it uses a very simple integer address-based genetic representation of a program in the form of a directed graph. Graphs are very useful program representations and can be applied to many domains (e.g. electronic circuits, neural networks). It can handle cyclic or acyclic graphs. In a number of studies, CGP has been shown to be comparatively efficient to other GP techniques. It is also very simple to program. The classical form of CGP has undergone a number of developments which have made it more useful, efficient and flexible in various ways. These include self-modifying CGP (SMCGP), cyclic connections (recurrent-CGP), encoding artificial neural networks and automatically defined functions (modular CGP).SMCGP uses functions that cause the evolved programs to change themselves as a function of time. This makes it possible to find general solutions to classes of problems and mathematical algorithms (e.g. arbitrary parity, n-bit binary addition, sequences that provably compute pi and e to arbitrary precision, and so on).Recurrent-CGP allows evolution to create programs which contain cyclic, as well as acyclic, connections. This enables application to tasks which require internal states or memory. It also allows CGP to create recursive equations.CGP encoded artificial neural networks represent a powerful training method for neural networks. This is because CGP is able to simultaneously evolve the networks connections weights, topology and neuron transfer functions. It is also compatible with Recurrent-CGP enabling the evolution of recurrent neural networks.The tutorial will cover the basic technique, advanced developments and applications to a variety of problem domains. It will present a live demo of how the open source cgplibrary can be used.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {179–198},
numpages = {20},
keywords = {automatic programming, evolutionary computation, genetic programming},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@article{es_intro,
author = {Beyer, Hans-Georg and Schwefel, Hans-Paul},
year = {2002},
month = {03},
pages = {3-52},
title = {Evolution strategies - A comprehensive introduction},
volume = {1},
journal = {Natural Computing},
}

@misc{challenges_of_rl,
      title={Challenges of Real-World Reinforcement Learning},
      author={Gabriel Dulac-Arnold and Daniel Mankowitz and Todd Hester},
      year={2019},
      eprint={1904.12901},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{rl_control,
title = {Reinforcement learning for building controls: The opportunities and challenges},
journal = {Applied Energy},
volume = {269},
pages = {115036},
year = {2020},
author = {Zhe Wang and Tianzhen Hong}
}

@misc{llama3,
   author = "Meta",
   title = "ntroducing Meta Llama 3: The most capable openly available LLM to date",
   year = "2024",
   howpublished = {\url{https://ai.meta.com/blog/meta-llama-3/}}
}

@article{neuroevolution_learning,
author = {Floreano, Dario and Mattiussi, Claudio},
year = {2008},
month = {03},
pages = {},
title = {Neuroevolution: From architectures to learning},
volume = {1},
journal = {Evol Intell},
}

@ARTICLE{hyperneat,
  author={Stanley, Kenneth O. and D'Ambrosio, David B. and Gauci, Jason},
  journal={Artificial Life},
  title={A Hypercube-Based Encoding for Evolving Large-Scale Neural Networks},
  year={2009},
  volume={15},
  number={2},
  pages={185-212},
  keywords={Compositional pattern-producing networks;CPPNs;HyperNEAT;indirect encoding;hypercube-based NeuroEvolution of Augmenting Topologies;artificial embryogeny},
}

@inproceedings{xnes,
author = {Glasmachers, Tobias and Schaul, Tom and Yi, Sun and Wierstra, Daan and Schmidhuber, J\"{u}rgen},
title = {Exponential natural evolution strategies},
year = {2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The family of natural evolution strategies (NES) offers a principled approach to real-valued evolutionary optimization by following the natural gradient of the expected fitness. Like the well-known CMA-ES, the most competitive algorithm in the field, NES comes with important invariance properties. In this paper, we introduce a number of elegant and efficient improvements of the basic NES algorithm. First, we propose to parameterize the positive definite covariance matrix using the exponential map, which allows the covariance matrix to be updated in a vector space. This new technique makes the algorithm completely invariant under linear transformations of the underlying search space, which was previously achieved only in the limit of small step sizes. Second, we compute all updates in the natural coordinate system, such that the natural gradient coincides with the vanilla gradient. This way we avoid the computation of the inverse Fisher information matrix, which is the main computational bottleneck of the original NES algorithm. Our new algorithm, exponential NES (xNES), is significantly simpler than its predecessors. We show that the various update rules in CMA-ES are closely related to the natural gradient updates of xNES. However, xNES is more principled than CMA-ES, as all the update rules needed for covariance matrix adaptation are derived from a single principle. We empirically assess the performance of the new algorithm on standard benchmark functions},
booktitle = {Proceedings of the 12th Annual Conference on Genetic and Evolutionary Computation},
pages = {393–400},
numpages = {8},
keywords = {unconstrained optimization, natural gradient, evolution strategies, black box optimization},
location = {Portland, Oregon, USA},
series = {GECCO '10}
}

@ARTICLE{suna,
  author={Vargas, Danilo Vasconcellos and Murata, Junichi},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  title={Spectrum-Diverse Neuroevolution With Unified Neural Models},
  year={2017},
  volume={28},
  number={8},
  pages={1759-1773},
  keywords={Neurons;Biological neural networks;Topology;Network topology;Biological cells;Encoding;Technological innovation;General artificial intelligence;neuroevolution;neuroEvolution of Augmenting Topology (NEAT);reinforcement learning;spectrum diversity;topology and weight evolving artificial neural network (TWEANN);unified neuron model},
}

@article{eant2,
author = {Siebel, Nils and Sommer, Gerald},
year = {2007},
month = {10},
pages = {171-183},
title = {Evolutionary reinforcement learning of artificial neural networks},
volume = {4},
journal = {Int. J. Hybrid Intell. Syst.},
}

@InProceedings{age,
author="D{\"u}rr, Peter
and Mattiussi, Claudio
and Floreano, Dario",
editor="Runarsson, Thomas Philip
and Beyer, Hans-Georg
and Burke, Edmund
and Merelo-Guerv{\'o}s, Juan J.
and Whitley, L. Darrell
and Yao, Xin",
title="Neuroevolution with Analog Genetic Encoding",
booktitle="Parallel Problem Solving from Nature - PPSN IX",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="671--680",
abstract="The evolution of artificial neural networks (ANNs) is often used to tackle difficult control problems. There are different approaches to the encoding of neural networks in artificial genomes. Analog Genetic Encoding (AGE) is a new implicit method derived from the observation of biological genetic regulatory networks. This paper shows how AGE can be used to simultaneously evolve the topology and the weights of ANNs for complex control systems. AGE is applied to a standard benchmark problem and we show that its performance is equivalent or superior to some of the most powerful algorithms for neuroevolution in the literature.",
}

@INPROCEEDINGS{cgpann,
  author={Khan, Maryam Mahsal and Khan, Gul Muhammad and Miller, Julian F.},
  booktitle={IEEE Congress on Evolutionary Computation},
  title={Evolution of neural networks using Cartesian Genetic Programming},
  year={2010},
  volume={},
  number={},
  pages={1-8},
  keywords={Neurons;Topology;Artificial neural networks;Network topology;Encoding;Genetic programming;Force},
}

@article{cosyne,
  title={Accelerated Neural Evolution through Cooperatively Coevolved Synapses},
  author={Faustino J. Gomez and J{\"u}rgen Schmidhuber and Risto Miikkulainen},
  journal={J. Mach. Learn. Res.},
  year={2008},
  volume={9},
  pages={937-965},
}

@misc{wanns,
      title={Weight Agnostic Neural Networks},
      author={Adam Gaier and David Ha},
      year={2019},
      eprint={1906.04358},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{mountain_car,
author = {Singh, Satinder and Sutton, Richard and Kaelbling, P.},
year = {1995},
month = {11},
pages = {},
title = {Reinforcement Learning with Replacing Eligibility Traces},
volume = {22},
journal = {Machine Learning},
}

@inproceedings{ball_throwing,
author = {Koutník, Jan and Gomez, Faustino and Schmidhuber, Jürgen},
year = {2010},
month = {07},
pages = {619-626},
title = {Evolving Neural Networks in Compressed Weight Space},
}

@INPROCEEDINGS{neuroevolution_es_rl,
  author={Igel, C.},
  booktitle={The 2003 Congress on Evolutionary Computation, 2003. CEC '03.},
  title={Neuroevolution for reinforcement learning using evolution strategies},
  year={2003},
  volume={4},
  number={},
  pages={2588-2595 Vol.4},
  keywords={Learning;Neural networks;Evolutionary computation;Genetic mutations;Optimization methods;Search methods;Stochastic processes;Covariance matrix;Network topology;Delay},
}

@article{robust,
   title={Robust optimization through neuroevolution},
   volume={14},
   ISSN={1932-6203},
   number={3},
   journal={PLOS ONE},
   publisher={Public Library of Science (PLoS)},
   author={Pagliuca, Paolo and Nolfi, Stefano},
   editor={Zeng, Ziqiang},
   year={2019},
   month=mar, pages={e0213193} }

@misc{spherical,
   author = "Wikipedia",
   title = "{N-sphere} --- {W}ikipedia{,} The Free Encyclopedia",
   year = "2024",
   howpublished = {\url{http://en.wikipedia.org/w/index.php?title=N-sphere&oldid=1226047223}},
   note = "[Online; accessed 03-June-2024]"
}

@misc{euler,
   author = "Wikipedia",
   title = "{Euler method} --- {W}ikipedia{,} The Free Encyclopedia",
   year = "2024",
   howpublished = {\url{http://en.wikipedia.org/w/index.php?title=Euler\%20method&oldid=1227210237}},
   note = "[Online; accessed 05-June-2024]"
 }

@article{smac,
   author  = {Marius Lindauer and Katharina Eggensperger and Matthias Feurer and André Biedenkapp and Difan Deng and Carolin Benjamins and Tim Ruhkopf and René Sass and Frank Hutter},
   title   = {SMAC3: A Versatile Bayesian Optimization Package for Hyperparameter Optimization},
   journal = {Journal of Machine Learning Research},
   year    = {2022},
   volume  = {23},
   number  = {54},
   pages   = {1--9},
}

@article{irace,
title = {The irace package: Iterated racing for automatic algorithm configuration},
journal = {Operations Research Perspectives},
volume = {3},
pages = {43-58},
year = {2016},
issn = {2214-7160},
author = {Manuel López-Ibáñez and Jérémie Dubois-Lacoste and Leslie {Pérez Cáceres} and Mauro Birattari and Thomas Stützle},
keywords = {Automatic algorithm configuration, Racing, Parameter tuning},
abstract = {Modern optimization algorithms typically require the setting of a large number of parameters to optimize their performance. The immediate goal of automatic algorithm configuration is to find, automatically, the best parameter settings of an optimizer. Ultimately, automatic algorithm configuration has the potential to lead to new design paradigms for optimization software. The irace package is a software package that implements a number of automatic configuration procedures. In particular, it offers iterated racing procedures, which have been used successfully to automatically configure various state-of-the-art algorithms. The iterated racing procedures implemented in irace include the iterated F-race algorithm and several extensions and improvements over it. In this paper, we describe the rationale underlying the iterated racing procedures and introduce a number of recent extensions. Among these, we introduce a restart mechanism to avoid premature convergence, the use of truncated sampling distributions to handle correctly parameter bounds, and an elitist racing procedure for ensuring that the best configurations returned are also those evaluated in the highest number of training instances. We experimentally evaluate the most recent version of irace and demonstrate with a number of example applications the use and potential of irace, in particular, and automatic algorithm configuration, in general.}
}

@article{automl,
   title={AutoML: A survey of the state-of-the-art},
   volume={212},
   ISSN={0950-7051},
   journal={Knowledge-Based Systems},
   publisher={Elsevier BV},
   author={He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
   year={2021},
   month=jan, pages={106622} }

@misc{wikipedia_cmaes,
   author = "Wikipedia",
   title = "{CMA-ES} --- {W}ikipedia{,} The Free Encyclopedia",
   year = "2024",
   howpublished = {\url{http://en.wikipedia.org/w/index.php?title=CMA-ES&oldid=1225990006}},
   note = "[Online; accessed 09-June-2024]"
}
