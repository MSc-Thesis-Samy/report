\chapter{Literature Review}
\label{chap:review}

...

\section{Neuroevolution algorithms}

\subsection{(1 + 1) NA}

The (1 + 1) NA algorithm and its variants were introduced in \cite{na}.
In this work, the authors consider a simple neuroevolution setting where these algorithms are used to optimize the weights and activation function of
a simple artificial neural network.

\subsubsection{The artificial neural network topology}

Artificial neurons with $D$ inputs and a binary threshold activation function are considered.
These neurons have $D$ parameters, the input weights $w_1, \ldots, w_D$ and the threshold $t$.
Let $x = (x_1, \ldots, x_D) \in \mathds{R}^D$ be the input of the neuron. The neuron outputs $1$ if $\sum_{i=1}^D w_i x_i \geq t$ and $0$
otherwise.
This can be interpreted geometrically as the neuron outputting $1$ if the input vector $x$ is above or on the hyperplane with normal vector
$w = (w_1, \ldots, w_D)$ and bias $t$.
j
Furthermore, an alternative representation of the decision hyperplane can be used by considering spherical coordinates.
The normal vector to the decision hyperplane is described by by $D - 1$ angles and the bias.
As a matter of fact, for $D = 2$, the normal vector can be represented by its cartesian coordinates $(x_1, x_2)$ or by its polar coordinates
$(r, \theta)$, where $r$ is the distance from the origin and $\theta$ is the angle with the $x_1$ axis. Similarly, for $D = 3$, the normal vector can
be represented by its cartesian coordinates $(x_1, x_2, x_3)$ or by its spherical coordinates $(r, \theta, \phi)$, where $r$ is the distance from the
origin, $\theta$ is the angle with the $x_1$ axis and $\phi$ is the angle with the $x_3$ axis.
It is easy to convert between these two representations. In addition, the spherical representation uses one less parameter than the cartesian
representation, and hence, allows for the reduction of the number of inputs to the neurons to $D - 1$.

% TODO coordinates figure

The ANNs which are considered in the study contain two layers, a hidden layer with $N > 1$ neurons and an output layer with a single neuron.
Each of the hidden neurons are connected to the $D$ inputs and output a binary value. The output neuron is connected to the $N$ hidden neurons and
computes the Boolean OR function of their outputs.
This architecture is motivated by the problems which are considered in the study, described in \Cref{subsec:na-problems}.
Geometrically, these ANNs output the union of a number of $N$-dimensional hyperplanes.

% TODO ANN figure

\subsubsection{The (1 + 1) NA algorithm}

Let's consider an ANN with $N$ neurons in the hidden layer and $D$ inputs, with parameters
$(\phi_{1,1}, \dots, \phi_{1,D-1}, b_1, \dots, \phi_{N,1}, \dots, \phi_{N,D-1}, b_N$.
In the paper \cite{na}, the search space $[0 , \dots, n]^{N D}$ is considered, where $r$ is the resolution of the continuous $[0, 1]$ domain.
This discretiisation allows for the values $\{0, 1/r, 2/r, \dots, 1\}$. Setting the parameters of ANNs is tipically a continous optimization problem,
but rigurous runtime analysis is much less developed for continous optimization than for discrete optimization, which motivates this choice.
Let $f : \{0 , \dots, r\}^{N D} \to [0, 1]$ be the fitness function which measures the performance of the ANN and is to be maximized.

The (1 + 1) NA algorithm is given in \Cref{alg:na}.
It maintains a single individual and mutates all angles and biases independently, based on a global search operator using the harmonic distribution
$\text{Harm}(r)$ on $\{1, \dots, r\}$: For $l \sim \text{Harm}(r)$,

\[
    Prob(l = i) = \frac{1}{H_r} \text{ for } i = 1, \dots, r, \text{ where } H_r = \sum_{i=1}^r \frac{1}{i}.
\]

\begin{algorithm}
\caption{(1 + 1) NA}
\label{alg:na}
\begin{algorithmic}
    \State $t \gets 0$
    \State Select $x_0$ uniformly at random from $\{0, \ldots, r\}^{DN}$.
    \While{termination criterion not met}
        \State Let $y = (\varphi_{1,1}, \ldots, \varphi_{1,D-1}, b_1, \ldots, \varphi_{N,1}, \ldots, \varphi_{N,N-1}, b_N) \gets x_t;$
        \ForAll{$i \in \{1, \ldots, N\}$}
            \State Mutate $\varphi_i$ and $b_i$ with probability $\frac{1}{DN}$, independently of each other and other indices;
            \State Mutation chooses $\sigma \in \{-1, 1\}$ uniformly at random and $l \sym \text{Harm}(r)$ and adds $\sigma l$ to the selected component, the
            result is then taken modulo $r$ for angles and modulo $r + 1$ for biases.
            \For {$i \in \{1, \ldots, N\}$}
                \State Set bias $2b_i / r - 1$ for neuron $i$.
                \For {$j \in \{1, \ldots, D\}$}
                    \State Set the $j$-th polar angle to $2\pi \varphi_{i,j} / r$ for neuron $i$.
                \EndFor
            \EndFor
            \State Evaluate $f(y)$
            \If{$f(y) \geq f(x_t)$}
                \State $x_{t+1} \gets y$
            \Else
                \State $x_{t+1} \gets x_t$
            \EndIf
        \EndFor
        \State $t \gets t + 1$
    \EndWhile
\end{algorithmic}
\end{algorithm}

\subsection{Bias-Invariant (1+1) NA (BNA)}

In \cite{bna}, the authors extend upon the analysis in \cite{na} by considering more realistic ANN settings, presenting the Bias-Invariant (1+1) NA (BNA) algorithm.
The considered ANNs uses Rectified-Linear-Unit (ReLU) activation functions, commonly used in real-world ANNs.
This allows for the construction of bended hyperplanes, resulting in solutions to the problems described in \Cref{subsec:na-problems} which are invariant
to the bias.

\subsubsection{The artificial neural network topology}

The considered ANNs contain three layers, in which each of the neurons uses a ReLU activation function i.e they output $\max(0, \sum_{i=1}^k w_i x_i)$ for
$k$ inputs from the previous layer.
The weights between the first and second layer and between the second and third layer are fixed. The topology for $D = 2$ in shown in \Cref{fig:bna-ann}.
The use of ReLU activation functions results in piecewise linear output. Hence, as described in \Cref{fig:bna-ann} for the case $D = 2$, these networks compute a
V-shaped area of positive classification. Such topologies are considered as a single neuron, refered to as a \textbf{V-neuron}, and which can be part of a
standard ANN topology.

Therefore, these V-neurons can be described by $D + 1$ parameters:

\begin{itemize}
    \item The bias $b$
    \item The $D - 1$ angles $\varphi_1, \ldots, \varphi_{D-1}$.
    \item The bend angle $\theta$.
\end{itemize}

The area of positive classification is a (multi-dimensional) cone, all points positively classified correspond to points forming an angle smaller than the bend
angle $\theta$ with the normal vector to the hyperplane given by the bias $b$ and the $D -1$ angles $\varphi_1, \ldots, \varphi_{D-1}$.

% TODO ANN figure
% TODO v-neuron classification figure

\subsubsection{The Bias-Invariant (1+1) NA algorithm}

The BNA algorithm is given in \Cref{alg:bna}.
It is mostly the same as the (1+1) NA algorithm, with the difference that the bend angles are also mutated.

\begin{algorithm}
    \caption{Bias-Invariant (1 + 1) NA (BNA)}
\label{alg:bna}
\begin{algorithmic}
    \State $t \gets 0$
    \State Select $x_0$ uniformly at random from $\{0, \ldots, r\}^{DN}$.
    \While{termination criterion not met}
        \State Let $y = (\theta_1, \varphi_{1,1}, \ldots, \varphi_{1,D-1}, b_1, \ldots, \theta_N, \varphi_{N,1}, \ldots, \varphi_{N,N-1}, b_N) \gets x_t;$
        \ForAll{$i \in \{1, \ldots, N\}$}
            \State Mutate $\varphi_i$ and $b_i$ with probability $\frac{1}{(D+1) N}$, independently of each other and other indices;
            \State Mutation chooses $\sigma \in \{-1, 1\}$ uniformly at random and $l \sym \text{Harm}(r)$ and adds $\sigma l$ to the selected component, the
            result is then taken modulo $r$ for angles and modulo $r + 1$ for biases.
            \For {$i \in \{1, \ldots, N\}$}
                \State Set bias $2b_i / r - 1$ for neuron $i$.
                \State Set bend angle $\pi \theta_i / r$ for neuron $i$.
                \For {$j \in \{1, \ldots, D\}$}
                    \State Set the $j$-th polar angle to $2\pi \varphi_{i,j} / r$ for neuron $i$.
                \EndFor
            \EndFor
            \State Evaluate $f(y)$
            \If{$f(y) \geq f(x_t)$}
                \State $x_{t+1} \gets y$
            \Else
                \State $x_{t+1} \gets x_t$
            \EndIf
        \EndFor
        \State $t \gets t + 1$
    \EndWhile
\end{algorithmic}
\end{algorithm}


\section{Neuroevolution benchmarks}

\subsection{Unit hypersphere sphere classification problems}

%TODO add figures

These problems, which can be thought of as a kind of \textsc{onemax} for the (1 + 1) NA algorithm, were introduced in \cite{na}.
These problems consist in the binary classification of points in the $D$-dimensional unit hypersphere.

\paragraph{Half}
The \textsc{half} problem consists of all points with non-negative $x_D$ coordinate on the unit hypersphere:

\[
    \textsc{half} = \{x \in \mathds{R}^D, \lVert x \rVert_2 = 1 \text{ and } \varphi_{D-1} \in [0, \pi]\}.
\]

\paragraph{Quarter}
The \textsc{quarter} problem consists of all points with non-negative $x_{D-1}$ and $x_D$ coordinate on the unit hypersphere:

\[
    \textsc{quarter} = \{x \in \mathds{R}^D, \lVert x \rVert_2 = 1 \text{ and } \varphi_{D-1} \in [0, \pi / 2]\}.
\]

\paragraph{TwoQuarters}
The \textsc{twoquarters} problem consists of all points with either both negative or non-negative $x_{D-1}$ and $x_D$ coordinate on the unit hypersphere:

\[
    \textsc{twoquarters} = \{x \in \mathds{R}^D, \lVert x \rVert_2 = 1 \text{ and } \varphi_{D-1} \in [0, \pi / 2] \cup [\pi, 3\pi / 2]\}.
\]

\paragraph{LocalOpt}
The \textsc{localopt} problem consists of all points with polar angle $\varphi_{D-1}$ between 0 and 60, 120 and 180, 240 and 300 degrees:

\[
    \textsc{localopt} = \{x \in \mathds{R}^D, \lVert x \rVert_2 = 1 \text{ and } \varphi_{D-1} \in [0, \pi / 3] \cup [2\pi / 3, \pi] \cup [4\pi / 3, 5\pi / 3]\}.
\]

% Square and cube
